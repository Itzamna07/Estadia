{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ac9xtqTsP4cc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapping negativas\n"
      ],
      "metadata": {
        "id": "ac9xtqTsP4cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNncIzPjF8to",
        "outputId": "383a305f-51bd-4243-9283-503528fcb13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "#corpus = r'\\b(?:I|I’m|I am|Me|Myself|Mine)\\b'\n",
        "# URL de la página que deseas scrapear\n",
        "url = \"https://www.tumblr.com/galtx\"\n",
        "\n",
        "# Rutas de carpeta y csv\n",
        "folder_path = '/content/drive/My Drive/data_negativos'\n",
        "csv_file_path = os.path.join(folder_path, 'datos_negativos.csv')\n",
        "if os.path.exists(csv_file_path):\n",
        "    mode = 'a'\n",
        "else:\n",
        "    mode = 'w'\n",
        "\n",
        "\n",
        "# Realiza una solicitud GET a la página\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    # Parsea el contenido de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    #print(soup)\n",
        "    articles = soup.select('div.So6RQ')\n",
        "    #print(articles)\n",
        "    data = []\n",
        "\n",
        "    # Itera a través de todos los elementos <article>\n",
        "    for article in articles:\n",
        "        texto_completo = \"\"\n",
        "        #print(article)\n",
        "\n",
        "        ge_yK = article.find('div', class_='ge_yK')\n",
        "        #print(ge_yK)\n",
        "\n",
        "        c79Av = ge_yK.find('div', class_='c79Av')\n",
        "        #print(c79Av)\n",
        "        FtjPK = c79Av.find('article', class_='FtjPK')\n",
        "        #print(FtjPK)\n",
        "\n",
        "        VDRZ4 = FtjPK.find('div', class_='VDRZ4')\n",
        "        #print(VDRZ4)\n",
        "\n",
        "        Qb2zX = VDRZ4.find('div','Qb2zX')\n",
        "        #print(Qb2zX)\n",
        "\n",
        "        span = Qb2zX.find('span')\n",
        "        #print(span)\n",
        "        GzjsW = span.find('div', 'GzjsW')\n",
        "        #print(GzjsW)\n",
        "\n",
        "        CQmeg = GzjsW.find('div', 'CQmeg')\n",
        "        #print(CQmeg)\n",
        "\n",
        "        if CQmeg:\n",
        "            TRX6J = CQmeg.find('button', 'TRX6J')\n",
        "            #print(TRX6J)\n",
        "            EvhBA = TRX6J.find('span', 'EvhBA')\n",
        "            #print(EvhBA)\n",
        "\n",
        "            DdFPj = EvhBA.find('figure', 'DdFPj')\n",
        "            #print(DdFPj)\n",
        "            HsI7c =  DdFPj.find('div', 'HsI7c')\n",
        "            #print(HsI7c)\n",
        "\n",
        "            RoN4R = HsI7c.find('img', 'RoN4R')\n",
        "            #print(RoN4R)\n",
        "\n",
        "            if RoN4R:\n",
        "              srcset = RoN4R['srcset']\n",
        "              # Divide el srcset en diferentes URLs usando ',' como separador\n",
        "              urls = srcset.split(',')\n",
        "\n",
        "              url = urls[5].strip().split(' ')[0]\n",
        "              print(url)\n",
        "              # URL DE IMAGEN OBTENIDO\n",
        "\n",
        "              #Descargar imagen del url obtenido\n",
        "              response_image = requests.get(url)\n",
        "              if response_image.status_code == 200:\n",
        "\n",
        "                  image = Image.open(BytesIO(response_image.content))\n",
        "                  filename = re.search(r'/([a-f\\d]+)\\.(jpg|png|jpeg|pnj)', url)\n",
        "                  file_extension = filename.group(2)\n",
        "                  if file_extension == 'pnj':\n",
        "                      file_extension = 'png'\n",
        "                  #NOMBRE DE IMAGEN OBTENIDO\n",
        "                  image_path = os.path.join(folder_path, f'{filename.group(1)}.{file_extension}')\n",
        "                  image.save(image_path)\n",
        "                  #print(f'Imagen descargada y guardada como: {image_path}')\n",
        "              else:\n",
        "                  print(\"Error al descargar la imagen. Código de estado:\", response_image.status_code)\n",
        "            else:\n",
        "              print(\"publicacion con otro formato de imagenes\")\n",
        "            texto_completo = \"\"\n",
        "            divs_internos = GzjsW.find_all('div')\n",
        "\n",
        "            for div in divs_internos:\n",
        "                texto = div.get_text(strip=True)\n",
        "                texto_completo += texto\n",
        "                #obtencion del texto completo de la publicacion\n",
        "\n",
        "            print(texto_completo)\n",
        "\n",
        "            #Obtencion URLs\n",
        "            qYXF9 = FtjPK.find('div', 'qYXF9')\n",
        "            #print(qYXF9)\n",
        "            hAFp3 = qYXF9.find('div', 'hAFp3')\n",
        "            #print(hAFp3)\n",
        "            mwjNz = hAFp3.find('div', 'mwjNz')\n",
        "            #print(mwjNz.prettify())\n",
        "\n",
        "            lista_hashtags= []\n",
        "\n",
        "            hashtags = mwjNz.find_all('a', 'KSDLH')\n",
        "            #print(hashtags)\n",
        "\n",
        "            for hashtag in hashtags:\n",
        "                hashtag_text = hashtag.text.strip()\n",
        "                lista_hashtags.append(hashtag_text)\n",
        "\n",
        "            #HASHTAGS OBTENIDOS\n",
        "            hashtags_string = ', '.join(lista_hashtags)\n",
        "            print(hashtags_string)\n",
        "\n",
        "            data.append([filename.group(1) + '.' + filename.group(2), texto_completo, hashtags_string])\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"Publicacion sin imagen\")\n",
        "\n",
        "\n",
        "        #palabras_encontradas = re.findall(corpus, texto_completo, flags=re.IGNORECASE)\n",
        "\n",
        "        # Verificar si se encontraron palabras del corpus\n",
        "        #if palabras_encontradas:\n",
        "            #print(\"Se encontraron palabras del corpus en el texto:\", palabras_encontradas)\n",
        "            #obtner info de la publicacion\n",
        "        #else:\n",
        "            #print(\"No se encontraron palabras del corpus en el texto.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error al realizar la solicitud a la página:\", response.status_code)\n",
        "\n",
        "with open(csv_file_path, mode, newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    if mode == 'w':\n",
        "        csv_writer.writerow(['nombre', 'texto', 'hashtags'])\n",
        "\n",
        "    # Escribe los datos\n",
        "    csv_writer.writerows(data)\n",
        "print(f'Datos guardados en {csv_file_path}')\n"
      ],
      "metadata": {
        "id": "r2MT3ehgMNLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping positivas"
      ],
      "metadata": {
        "id": "5N5OyV6Kac5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LNgk3Bhyahy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "corpus = r'\\b(?:I|I’m|I am|Me|Myself|Mine)\\b'\n",
        "# URL de la página que deseas scrapear\n",
        "url = \"https://www.tumblr.com/dancingcrowley\"\n",
        "\n",
        "# Rutas de carpeta y csv\n",
        "folder_path = '/content/drive/My Drive/data_positivos'\n",
        "csv_file_path = os.path.join(folder_path, 'datos_positivos.csv')\n",
        "if os.path.exists(csv_file_path):\n",
        "    mode = 'a'\n",
        "else:\n",
        "    mode = 'w'\n",
        "\n",
        "# Realiza una solicitud GET a la página\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    # Parsea el contenido de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    #print(soup)\n",
        "    articles = soup.select('div.So6RQ')\n",
        "    #print(articles)\n",
        "    data = []\n",
        "\n",
        "    # Itera a través de todos los elementos <article>\n",
        "    for article in articles:\n",
        "        texto_completo = \"\"\n",
        "        #print(article)\n",
        "\n",
        "        ge_yK = article.find('div', class_='ge_yK')\n",
        "        #print(ge_yK)\n",
        "\n",
        "        c79Av = ge_yK.find('div', class_='c79Av')\n",
        "        #print(c79Av)\n",
        "        FtjPK = c79Av.find('article', class_='FtjPK')\n",
        "        #print(FtjPK)\n",
        "\n",
        "        VDRZ4 = FtjPK.find('div', class_='VDRZ4')\n",
        "        #print(VDRZ4)\n",
        "\n",
        "        Qb2zX = VDRZ4.find('div','Qb2zX')\n",
        "        #print(Qb2zX)\n",
        "\n",
        "        span = Qb2zX.find('span')\n",
        "        #print(span)\n",
        "        GzjsW = span.find('div', 'GzjsW')\n",
        "        #print(GzjsW)\n",
        "\n",
        "        CQmeg = GzjsW.find('div', 'CQmeg')\n",
        "        #print(CQmeg)\n",
        "\n",
        "        #obtencion del texto\n",
        "        texto_completo = \"\"\n",
        "        divs_internos = GzjsW.find_all('div')\n",
        "\n",
        "        for div in divs_internos:\n",
        "            texto = div.get_text(strip=True)\n",
        "            texto_completo += texto\n",
        "            #texto completo de la publicacion obtneido\n",
        "\n",
        "        print(texto_completo)\n",
        "\n",
        "        palabras_encontradas = re.findall(corpus, texto_completo, flags=re.IGNORECASE)\n",
        "\n",
        "        # Verificar si se encontraron palabras del corpus\n",
        "        if palabras_encontradas:\n",
        "            print(\"Se encontraron palabras del corpus en el texto:\", palabras_encontradas)\n",
        "            #obtner info de la publicacion\n",
        "\n",
        "            # division de la imagen, si existe el POST contiene imagen\n",
        "            if CQmeg:\n",
        "                TRX6J = CQmeg.find('button', 'TRX6J')\n",
        "                #print(TRX6J)\n",
        "                EvhBA = TRX6J.find('span', 'EvhBA')\n",
        "                #print(EvhBA)\n",
        "\n",
        "                DdFPj = EvhBA.find('figure', 'DdFPj')\n",
        "                #print(DdFPj)\n",
        "                HsI7c =  DdFPj.find('div', 'HsI7c')\n",
        "                #print(HsI7c)\n",
        "\n",
        "                RoN4R = HsI7c.find('img', 'RoN4R')\n",
        "                #print(RoN4R)\n",
        "\n",
        "                if RoN4R:\n",
        "                  srcset = RoN4R['srcset']\n",
        "                  # Divide el srcset en diferentes URLs usando ',' como separador\n",
        "                  urls = srcset.split(',')\n",
        "\n",
        "                  url = urls[5].strip().split(' ')[0]\n",
        "                  print(url)\n",
        "                  # URL DE IMAGEN OBTENIDO\n",
        "\n",
        "                  #Descargar imagen del url obtenido\n",
        "                  response_image = requests.get(url)\n",
        "                  if response_image.status_code == 200:\n",
        "\n",
        "                      image = Image.open(BytesIO(response_image.content))\n",
        "                      filename = re.search(r'/([a-f\\d]+)\\.(jpg|png|jpeg|pnj)', url)\n",
        "                      file_extension = filename.group(2)\n",
        "                      if file_extension == 'pnj':\n",
        "                          file_extension = 'png'\n",
        "                      #NOMBRE DE IMAGEN OBTENIDO\n",
        "                      image_path = os.path.join(folder_path, f'{filename.group(1)}.{file_extension}')\n",
        "                      image.save(image_path)\n",
        "                      #print(f'Imagen descargada y guardada como: {image_path}')\n",
        "                  else:\n",
        "                      print(\"Error al descargar la imagen. Código de estado:\", response_image.status_code)\n",
        "\n",
        "\n",
        "                  #Obtencion URLs\n",
        "                  qYXF9 = FtjPK.find('div', 'qYXF9')\n",
        "                  #print(qYXF9)\n",
        "                  hAFp3 = qYXF9.find('div', 'hAFp3')\n",
        "                  #print(hAFp3)\n",
        "                  mwjNz = hAFp3.find('div', 'mwjNz')\n",
        "                  #print(mwjNz.prettify())\n",
        "\n",
        "                  lista_hashtags= []\n",
        "\n",
        "                  hashtags = mwjNz.find_all('a', 'KSDLH')\n",
        "                  #print(hashtags)\n",
        "\n",
        "                  for hashtag in hashtags:\n",
        "                      hashtag_text = hashtag.text.strip()\n",
        "                      lista_hashtags.append(hashtag_text)\n",
        "\n",
        "                  #HASHTAGS OBTENIDOS\n",
        "                  hashtags_string = ', '.join(lista_hashtags)\n",
        "                  print(hashtags_string)\n",
        "\n",
        "                  data.append([filename.group(1) + '.' + filename.group(2), texto_completo, hashtags_string])\n",
        "                else:\n",
        "                  print(\"publicacion con otro formato de imagenes\")\n",
        "            else:\n",
        "              print(\"Publicacion sin imagen\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"No se encontraron palabras del corpus en el texto, no se guardara publicacion\")\n",
        "\n",
        "else:\n",
        "    print(\"Error al realizar la solicitud a la página:\", response.status_code)\n",
        "\n",
        "with open(csv_file_path, mode, newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    if mode == 'w':\n",
        "        csv_writer.writerow(['nombre', 'texto', 'hashtags'])\n",
        "\n",
        "    # Escribe los datos\n",
        "    csv_writer.writerows(data)\n",
        "print(f'Datos guardados en {csv_file_path}')\n"
      ],
      "metadata": {
        "id": "AuSUYPqnfU-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#scrap tumblr"
      ],
      "metadata": {
        "id": "qb6jKNs2DC8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la página que deseas scrape\n",
        "url = \"https://javascript.tumblr.com/\"\n",
        "\n",
        "# Realiza una solicitud GET a la página\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsea el contenido de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encuentra todas las etiquetas 'a' con la clase 'u-photo'\n",
        "    image_links = soup.find_all('a', class_='u-photo')\n",
        "\n",
        "    # Itera a través de cada enlace de imagen\n",
        "    for link in image_links:\n",
        "        # Extrae la URL de la imagen\n",
        "        image_url = link.find('img')['src']\n",
        "\n",
        "        # Extrae la descripción de la imagen del atributo 'alt' de la etiqueta 'img'\n",
        "        image_description = link.find('img')['alt']\n",
        "\n",
        "        # Imprime la información\n",
        "        print(\"URL de la imagen:\", image_url)\n",
        "        print(\"Descripción de la imagen:\", image_description)\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"Error al realizar la solicitud a la página:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "8CExARvfQWhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la página de Tumblr que deseas scrape\n",
        "url = \"https://galtx.tumblr.com\"\n",
        "\n",
        "# Realiza una solicitud GET a la página\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsea el contenido de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encuentra todos los elementos <article> en la página\n",
        "    articles = soup.find_all('article')\n",
        "\n",
        "    # Itera a través de los elementos <article> y extrae la información deseada\n",
        "    for article in articles:\n",
        "        # Aquí puedes realizar operaciones adicionales para extraer datos específicos de cada artículo\n",
        "        # Por ejemplo, puedes obtener el texto dentro de cada artículo usando article.text\n",
        "        print(article.text)\n",
        "else:\n",
        "    print(\"Error al cargar la página:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "LtQawHtlDCkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la página que deseas scrape\n",
        "url = \"https://www.tumblr.com/galtx\"\n",
        "\n",
        "# Realiza una solicitud GET a la página\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsea el contenido de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    #print(soup)\n",
        "\n",
        "    # Encuentra todos los elementos <article> dentro de la sección \"main\"\n",
        "    articles = soup.select('div.root body')\n",
        "    print(articles)\n",
        "    # Itera a través de todos los elementos <article>\n",
        "    for article in articles:\n",
        "        print(article)\n",
        "        # Ingresa a las clases internas para obtener la URL de la imagen y la descripción\n",
        "        post_wrapper = article.find('div', class_='post-wrapper')\n",
        "        print(post_wrapper)\n",
        "        post_section = post_wrapper.find('section', class_='post')\n",
        "        #print(post_section)\n",
        "        post_content = post_section.find('div', class_='post-content')\n",
        "        #print(post_content)\n",
        "        image_title = post_figure.find('h2', class_='title')\n",
        "        #print(image_title)\n",
        "        image_link = image_title.find('a', class_='u-url')['href']\n",
        "        #print(image_link)\n",
        "\n",
        "        image_body = post_content.find('div', class_='body-text')\n",
        "        #print(image_body)\n",
        "        paragraph_text = image_body.get_text()\n",
        "        #print(paragraph_text)\n",
        "\n",
        "else:\n",
        "    print(\"Error al realizar la solicitud a la página:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm2jZOQR-FL0",
        "outputId": "397a5976-2148-4ae9-9a53-66c03db8af69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as re\n",
        "\n",
        "response = re.get('https://www.tumblr.com/galtx')\n",
        "html = response.text\n",
        "\n",
        "print(html)"
      ],
      "metadata": {
        "id": "tvsipIC_fQig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}