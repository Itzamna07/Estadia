{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSleC7bvD1mc"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YURJzbZXGIGV"
      },
      "outputs": [],
      "source": [
        "## importing the tokenizer and subword BPE trainer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
        "                                WordPieceTrainer, UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwm2JdqKGKvi",
        "outputId": "7ee7e490-b33d-4081-c648-813119662665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-06 17:06:51--  http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/16457/pg16457.txt [following]\n",
            "--2023-12-06 17:06:51--  https://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 617780 (603K) [text/plain]\n",
            "Saving to: ‘pg16457.txt.1’\n",
            "\n",
            "pg16457.txt.1       100%[===================>] 603.30K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-12-06 17:07:12 (6.94 MB/s) - ‘pg16457.txt.1’ saved [617780/617780]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrig9gHRD5hs",
        "outputId": "9e6118cf-d603-4247-ad33-be213347bb3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-06 17:07:12--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.54.64, 52.216.44.120, 52.217.90.214, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.54.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip.1’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  61.8MB/s    in 3.0s    \n",
            "\n",
            "2023-12-06 17:07:16 (61.8 MB/s) - ‘wikitext-103-raw-v1.zip.1’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "replace wikitext-103-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI-1dUWZD8DS"
      },
      "outputs": [],
      "source": [
        "unk_token = \"<UNK>\"  # token for unknown words\n",
        "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
        "\n",
        "def prepare_tokenizer_trainer(alg):\n",
        "    \"\"\"\n",
        "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
        "    \"\"\"\n",
        "    if alg == 'BPE':\n",
        "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
        "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
        "    elif alg == 'UNI':\n",
        "        tokenizer = Tokenizer(Unigram())\n",
        "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
        "    elif alg == 'WPC':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
        "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
        "\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    return tokenizer, trainer\n",
        "\n",
        "\n",
        "def train_tokenizer(files, alg='WLV'):\n",
        "    \"\"\"\n",
        "    Takes the files and trains the tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
        "    tokenizer.train(files, trainer) # training the tokenzier\n",
        "    tokenizer.save(\"./tokenizer-trained.json\")\n",
        "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize(input_string, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes the input string using the tokenizer provided.\n",
        "    \"\"\"\n",
        "    output = tokenizer.encode(input_string)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA2_njFUD_Ey",
        "outputId": "2fd41d38-7b0a-4b56-89f0-27c19dbed018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========Using vocabulary from ['pg16457.txt']=======\n",
            "---- WLV ----\n",
            "['This', 'is', 'a', 'deep', 'learning', '<UNK>', '<UNK>', '.', '<UNK>', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', '<UNK>', '<UNK>', '.', 'We', 'will', 'be', 'comparing', 'the', '<UNK>', 'generated', 'by', 'each', '<UNK>', 'model', '.', '<UNK>', 'much', '<UNK>'] -> 35\n",
            "---- BPE ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 't', 'ut', 'or', 'ial', '.', 'T', 'ok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', 'k', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'c', 'ited', 'much', '?', '!', '<UNK>'] -> 55\n",
            "---- UNI ----\n",
            "['Thi', 's', 'is', 'a', 'deep', 'learn', 'ing', 'to', 'ken', 'iz', 'ation', 't', 'u', 'to', 'rial', '.', 'To', 'ken', 'iz', 'ation', 'is', 'the', 'fir', 's', 't', 'step', 'in', 'a', 'deep', 'learn', 'ing', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'compar', 'ing', 'the', 'to', 'ken', 's', 'generat', 'ed', 'by', 'each', 'to', 'ken', 'iz', 'ation', 'model', '.', 'E', 'x', 'c', 'it', 'ed', 'm', 'uch', '?', '!', '😍'] -> 67\n",
            "---- WPC ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 't', '##ut', '##oria', '##l', '.', 'To', '##ken', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', '##L', '##P', 'pip', '##el', '##ine', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Ex', '##ci', '##ted', 'much', '<UNK>'] -> 52\n",
            "========Using vocabulary from ['./wikitext-103-raw/wiki.test.raw', './wikitext-103-raw/wiki.train.raw', './wikitext-103-raw/wiki.valid.raw']=======\n",
            "---- WLV ----\n",
            "['This', 'is', 'a', 'deep', 'learning', '<UNK>', '<UNK>', '.', '<UNK>', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', '<UNK>', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tokens', 'generated', 'by', 'each', '<UNK>', 'model', '.', '<UNK>', 'much', '<UNK>'] -> 35\n",
            "---- BPE ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 'tut', 'orial', '.', 'Tok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', 'P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tok', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'cited', 'much', '?', '!', '<UNK>'] -> 47\n",
            "---- UNI ----\n",
            "['This', 'i', 's', 'a', 'deep', 'learn', 'ing', 't', 'o', 'ken', 'ization', 't', 'u', 't', 'o', 'rial', '.', 'T', 'o', 'ken', 'ization', 'i', 's', 'the', 'first', 'step', 'in', 'a', 'deep', 'learn', 'ing', 'N', 'L', 'P', 'p', 'i', 'p', 'e', 'line', '.', 'W', 'e', 'will', 'be', 'com', 'par', 'ing', 'the', 't', 'o', 'ken', 's', 'generate', 'd', 'by', 'each', 't', 'o', 'ken', 'ization', 'model', '.', 'Ex', 'cited', 'much', '?', '!', '😍'] -> 68\n",
            "---- WPC ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 'tut', '##orial', '.', 'Tok', '##eni', '##za', '##ti', '##on', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', '##P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Exc', '##ited', 'much', '<UNK>'] -> 48\n"
          ]
        }
      ],
      "source": [
        "small_file = ['pg16457.txt']\n",
        "large_files = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for files in [small_file, large_files]:\n",
        "    print(f\"========Using vocabulary from {files}=======\")\n",
        "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
        "        trained_tokenizer = train_tokenizer(files, alg)\n",
        "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍\"\n",
        "        output = tokenize(input_string, trained_tokenizer)\n",
        "        tokens_dict[alg] = output.tokens\n",
        "        print(\"----\", alg, \"----\")\n",
        "        print(output.tokens, \"->\", len(output.tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyWWr5mgHzyC"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for alg in ['BPE', 'UNI', 'WPC']:\n",
        "    trained_tokenizer = train_tokenizer(large_files, alg)\n",
        "    input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍\"\n",
        "    output = tokenize(input_string, trained_tokenizer)\n",
        "    tokens_dict[alg] = output.tokens"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}