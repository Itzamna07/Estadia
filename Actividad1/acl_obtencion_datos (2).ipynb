{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqX3eEhi97wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b538170-0737-4cc3-c92b-1f1dec295633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Cargar entonrno de Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de dataset y obtencion de datos"
      ],
      "metadata": {
        "id": "KOmlf3GTxrMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "import json,re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part=[\"dev/parsed_pdfs/\",\"test/parsed_pdfs/\",\"train/parsed_pdfs/\"]\n",
        "lista_conf=[\"acl_2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    texto = re.sub(r'[×/]', '', texto)\n",
        "    texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    return texto\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "  #print(confe)\n",
        "  for particion in lista_part:\n",
        "    #print(particion)\n",
        "    ruta=(ruta_carpeta_principal+\"/\"+confe+\"/\"+particion)\n",
        "    #print(\"Explorando ruta:\", ruta)\n",
        "    archivos=glob.glob(ruta+\"/*.json\")\n",
        "    #print(archivos)\n",
        "    particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "    #print(particion_sin_ruta)\n",
        "\n",
        "    for archivo in archivos:\n",
        "      with open(archivo, 'r') as f:\n",
        "        contenido = json.load(f)\n",
        "        #print(contenido.keys())\n",
        "        nombre=contenido[\"name\"]\n",
        "        diccontent=contenido[\"metadata\"]\n",
        "        #print(diccontent)\n",
        "        t = diccontent[\"title\"]\n",
        "        #print(t)\n",
        "        e = diccontent[\"emails\"]\n",
        "        #print(e)\n",
        "        autores=diccontent[\"authors\"]\n",
        "        #print(autores)\n",
        "        dicsecciones=diccontent[\"sections\"]\n",
        "\n",
        "        referencias_archivo = diccontent[\"references\"]\n",
        "\n",
        "        abstract= diccontent[\"abstractText\"]\n",
        "        #print(dicsecciones)\n",
        "        cadena=\"\"\n",
        "        introduccion=\"\"\n",
        "        conclusion=\"\"\n",
        "        contenido=\"\"\n",
        "\n",
        "        #print(dicsecciones)\n",
        "        if dicsecciones!=None:\n",
        "          for item in dicsecciones:\n",
        "            if item['heading'] == None:\n",
        "                cadena = \"\"\n",
        "                text=\"\"\n",
        "            else:\n",
        "                cadena = cadena +\" \"+ item['heading'] + \"\\n\"\n",
        "                cadena += \" \" #limpiar_texto(cadena)+\" \"\n",
        "\n",
        "                # Verificar si el heading coincide con el patron de introduccion y concllu\n",
        "                if re.search(intro_pattern, item['heading']):\n",
        "                  introduccion = item['text'] #limpiar_texto(item['text'])\n",
        "                elif re.search(conclusion_pattern, item['heading']):\n",
        "                  conclusion = item['text'] #limpiar_texto(item['text'])\n",
        "                else:\n",
        "                  text= item['text']# limpiar_texto(item['text'])\n",
        "                  contenido += f\"**{item['heading']}**\\n\"+ text+ \"\\n\"\n",
        "\n",
        "            texto = item['text'] #re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            #texto = limpiar_texto(texto)\n",
        "            cadena = cadena+\" \"+texto+\"\\n\"\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      #\"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": f\"{t} {cadena} Abstract {abstract}\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": introduccion,\n",
        "                      \"Contenido\": contenido,\n",
        "                      \"Conclusion\": conclusion,\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "        else:\n",
        "          print(\"Error en el archivo: \"+nombre)\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": \"\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": \"\",\n",
        "                      \"Contenido\": \"\",\n",
        "                      \"Conclusion\": \"\",\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "df.to_csv(ruta_carpeta_principal+\"/\"+\"acl_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal+\"/\"+\"acl_primer_ciclo.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWdNJfnQ-L_y",
        "outputId": "a9f32cc8-e45c-4300-ef2b-024426ff50fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/acl_primer_ciclo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import json, re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part = [\"dev/parsed_pdfs/\", \"test/parsed_pdfs/\", \"train/parsed_pdfs/\"]\n",
        "lista_conf = [\"acl_2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "# Contadores para las particiones\n",
        "contador_dev = 0\n",
        "contador_test = 0\n",
        "contador_train = 0\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    texto = re.sub(r'[×/]', '', texto)\n",
        "    texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    return texto\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "    for particion in lista_part:\n",
        "        ruta = (ruta_carpeta_principal + \"/\" + confe + \"/\" + particion)\n",
        "        archivos = glob.glob(ruta + \"/*.json\")\n",
        "        particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "\n",
        "        for archivo in archivos:\n",
        "            with open(archivo, 'r') as f:\n",
        "                contenido = json.load(f)\n",
        "                nombre = contenido[\"name\"]\n",
        "                #print(nombre)\n",
        "                diccontent = contenido[\"metadata\"]\n",
        "                t = diccontent[\"title\"]\n",
        "\n",
        "                e = diccontent[\"emails\"]\n",
        "                autores = diccontent[\"authors\"]\n",
        "                dicsecciones = diccontent[\"sections\"]\n",
        "                referencias_archivo = diccontent[\"references\"]\n",
        "                abstract = diccontent[\"abstractText\"]\n",
        "                cadena = \"\"\n",
        "                introduccion = \"\"\n",
        "                conclusion = \"\"\n",
        "                contenido = \"\"\n",
        "\n",
        "                if dicsecciones is not None:\n",
        "                    for item in dicsecciones:\n",
        "                        if item['heading'] is None:\n",
        "                            cadena = \"\"\n",
        "                            text = \"\"\n",
        "                        else:\n",
        "                            cadena = cadena + \" \" + item['heading'] + \"\\n\"\n",
        "                            cadena += \" \"\n",
        "                            if re.search(intro_pattern, item['heading']):\n",
        "                                introduccion = item['text']\n",
        "                            elif re.search(conclusion_pattern, item['heading']):\n",
        "                                conclusion = item['text']\n",
        "                            else:\n",
        "                                text = item['text']\n",
        "                                contenido += f\"**{item['heading']}**\\n\" + text + \"\\n\"\n",
        "\n",
        "                        texto = item['text']\n",
        "                        cadena = cadena + \" \" + texto + \"\\n\"\n",
        "                    rutas_archivos_json.append({\n",
        "                        \"Conferencia\": confe,\n",
        "                        \"Particion\": particion_sin_ruta,\n",
        "                        \"Archivo\": nombre,\n",
        "                        \"Title\": t,\n",
        "                        \"Emails\": e,\n",
        "                        \"Autores\": autores,\n",
        "                        \"Texto Completo\": f\"{t} {cadena} Abstract {abstract}\",\n",
        "                        \"Abstract\": abstract,\n",
        "                        \"Introducción\": introduccion,\n",
        "                        \"Contenido\": contenido,\n",
        "                        \"Conclusion\": conclusion,\n",
        "                        \"Referencias\": referencias_archivo\n",
        "                    })\n",
        "\n",
        "                    # Actualizar los contadores según la partición actual\n",
        "                    if particion_sin_ruta == \"dev\":\n",
        "                        contador_dev += 1\n",
        "                    elif particion_sin_ruta == \"test\":\n",
        "                        contador_test += 1\n",
        "                    elif particion_sin_ruta == \"train\":\n",
        "                        contador_train += 1\n",
        "                else:\n",
        "                    print(\"Error en el archivo: \" + nombre)\n",
        "                    rutas_archivos_json.append({\n",
        "                        \"Conferencia\": confe,\n",
        "                        \"Particion\": particion_sin_ruta,\n",
        "                        \"Archivo\": nombre,\n",
        "                        \"Title\": t,\n",
        "                        \"Emails\": e,\n",
        "                        \"Autores\": autores,\n",
        "                        \"Texto Completo\": \"\",\n",
        "                        \"Abstract\": abstract,\n",
        "                        \"Introducción\": \"\",\n",
        "                        \"Contenido\": \"\",\n",
        "                        \"Conclusion\": \"\",\n",
        "                        \"Referencias\": referencias_archivo\n",
        "                    })\n",
        "\n",
        "# Imprimir el número de archivos en cada partición\n",
        "#print(\"Archivos en la partición 'dev':\", contador_dev)\n",
        "#print(\"Archivos en la partición 'test':\", contador_test)\n",
        "#print(\"Archivos en la partición 'train':\", contador_train)\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "print(df)\n",
        "df.to_csv(ruta_carpeta_principal + \"/\" + \"acl_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal + \"/\" + \"acl_primer_ciclo.csv\")\n"
      ],
      "metadata": {
        "id": "SGyYl1uTsqU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener estado de aceptacion"
      ],
      "metadata": {
        "id": "OpSGsekgAeL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta del archivo\n",
        "archivo_ruta = '/content/drive/MyDrive/PeerRead-master/data/acl_accepted.txt'\n",
        "\n",
        "# Lista para almacenar los datos\n",
        "data = []\n",
        "\n",
        "# Leer el archivo\n",
        "with open(archivo_ruta, 'r') as archivo:\n",
        "    for linea in archivo:\n",
        "        # Agregar una nueva fila con 'Title' y 'Accepted' como TRUE\n",
        "        data.append({'Title': linea.strip(), 'Accepted': '1'})\n",
        "\n",
        "# Crear un DataFrame de Pandas\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Eliminar filas duplicadas basadas en la columna 'Title'\n",
        "df = df.drop_duplicates(subset='Title')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "print(df)\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "df.to_csv('/content/drive/MyDrive/PeerRead-master/data/reviews_acl.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Mf4BWv-N1z",
        "outputId": "17481ab8-958f-4453-a38a-a01a062f0cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                              Title  \\\n",
            "0                                               Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging   \n",
            "1                                                                         Finding Optimal 1-Endpoint-Crossing Trees   \n",
            "2                                                                           Grounding Action Descriptions in Videos   \n",
            "3                                         Branch and Bound Algorithm for Dependency Parsing with Non-local Features   \n",
            "4                                Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions   \n",
            "...                                                                                                             ...   \n",
            "19738                                                       Cross-Lingual Syntactic Transfer with Limited Resources   \n",
            "19739                                     Overcoming Language Variation in Sentiment Analysis with Social Attention   \n",
            "19740  Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints   \n",
            "19741                               Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding   \n",
            "19742                       Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation   \n",
            "\n",
            "      Accepted  \n",
            "0            1  \n",
            "1            1  \n",
            "2            1  \n",
            "3            1  \n",
            "4            1  \n",
            "...        ...  \n",
            "19738        1  \n",
            "19739        1  \n",
            "19740        1  \n",
            "19741        1  \n",
            "19742        1  \n",
            "\n",
            "[10406 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combinar csv"
      ],
      "metadata": {
        "id": "Z6fFy4WTH1dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"acl_miguel.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"reviews_acl.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Realiza la unión en base a la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Title', how='outer')\n",
        "\n",
        "# Llena las columnas 'Accepted' nulas con 'FALSE'\n",
        "result['Accepted'] = result['Accepted'].fillna('0')\n",
        "#result = result.dropna(subset=['Accepted'])\n",
        "result = result.dropna(subset=['Archivo'])\n",
        "\n",
        "print(result)\n",
        "# Guarda el resultado en un nuevo archivo CSV\n",
        "result.to_csv('/content/drive/MyDrive/PeerRead-master/data/acl_datos_miguel.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uaps5jQ2-PIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"acl_primer_ciclo.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"acl_datos_miguel.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Guarda el número inicial de filas en cada DataFrame\n",
        "num_filas_inicial_df1 = len(df1)\n",
        "print(\"Filas del doc 1:\",num_filas_inicial_df1)\n",
        "num_filas_inicial_df2 = len(df2)\n",
        "print(\"Filas del doc 2:\",num_filas_inicial_df2)\n",
        "\n",
        "# Union de csv con base en la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "print(\"Número total de elementos al unir:\", len(result))\n",
        "\n",
        "# Calcular la diferencia en el número de filas después de la unión\n",
        "#num_filas_eliminadas_despues_union = 0\n",
        "\n",
        "result['Accepted'] = result['Accepted'].fillna('0')\n",
        "\n",
        "# Dropear filas con valores nulos en columnas\n",
        "#result = result.dropna(subset=['Accepted'])\n",
        "#result = result.dropna(subset=['Contenido'])\n",
        "print(\"Número total de elementos despues de dropear:\", len(result))\n",
        "# Calcular la diferencia en el numero de filas despues de la limpieza\n",
        "\n",
        "#num_filas_eliminadas_despues_limpieza = 0  # No se eliminan filas en esta etapa ya que las líneas están comentadas\n",
        "#print(\"Número total de filas eliminadas en limpieza:\", num_filas_eliminadas_despues_limpieza)\n",
        "\n",
        "#print(result)\n",
        "# Guarda el resultado en un CSV\n",
        "result.to_csv('/content/drive/MyDrive/acl_datos_completos.csv', index=False)\n",
        "\n",
        "\n",
        "# Imprime el numero total de filas eliminadas\n",
        "#print(\"Número total de filas eliminadas:\", num_total_filas_eliminadas)\n",
        "\n"
      ],
      "metadata": {
        "id": "KxhSndNlEeHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}