{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRP-tSaD4sml"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de dataset y obtencion de datos\n"
      ],
      "metadata": {
        "id": "wESeph0T6N-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "import json,re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part=[\"dev/parsed_pdfs/\",\"test/parsed_pdfs/\",\"train/parsed_pdfs/\"]\n",
        "lista_conf=[\"arxiv.cs.ai_2007-2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    #texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    #texto = re.sub(r'[×/]', '', texto)\n",
        "    #texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    #texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    # Reemplazar caracteres problemáticos con un espacio en blanco\n",
        "    #texto_limpio = ''.join(char if ord(char) < 128 else ' ' for char in texto)\n",
        "    #return texto_limpio\n",
        "    # Reemplazar caracteres problemáticos con un espacio en blanco\n",
        "    texto_limpio = ''.join(char if char.isprintable() else ' ' for char in texto)\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "  print(confe)\n",
        "  for particion in lista_part:\n",
        "    #print(particion)\n",
        "    ruta=(ruta_carpeta_principal+\"/\"+confe+\"/\"+particion)\n",
        "    #print(\"Explorando ruta:\", ruta)\n",
        "    archivos=glob.glob(ruta+\"/*.json\")\n",
        "    #print(archivos)\n",
        "    particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "\n",
        "    for archivo in archivos:\n",
        "      with open(archivo, 'r') as f:\n",
        "        contenido = json.load(f)\n",
        "        #print(contenido.keys())\n",
        "        nombre=contenido[\"name\"]\n",
        "        diccontent=contenido[\"metadata\"]\n",
        "        #print(diccontent)\n",
        "        t = diccontent[\"title\"]\n",
        "        #print(t)\n",
        "        e = diccontent[\"emails\"]\n",
        "        #print(e)\n",
        "        autores=diccontent[\"authors\"]\n",
        "        #print(autores)\n",
        "        dicsecciones=diccontent[\"sections\"]\n",
        "\n",
        "        referencias_archivo = diccontent[\"references\"]\n",
        "        c = 0\n",
        "        abstract= diccontent[\"abstractText\"]\n",
        "        #print(dicsecciones)\n",
        "        cadena=\"\"\n",
        "        introduccion=\"\"\n",
        "        conclusion=\"\"\n",
        "        contenido=\"\"\n",
        "\n",
        "\n",
        "        #print(dicsecciones)\n",
        "        if dicsecciones!=None:\n",
        "\n",
        "          for item in dicsecciones:\n",
        "            if item['heading'] == None:\n",
        "                cadena = \"\"\n",
        "                text=\"\"\n",
        "            else:\n",
        "                cadena = cadena + item['heading'] + \"\\n\"\n",
        "                cadena = limpiar_texto(cadena) + \"\"\n",
        "\n",
        "                # Verificar si el heading coincide con el patron de introduccion y concllu\n",
        "                if re.search(intro_pattern, item['heading']):\n",
        "                  introduccion = limpiar_texto(item['text'])\n",
        "                elif re.search(conclusion_pattern, item['heading']):\n",
        "                  conclusion = limpiar_texto(item['text'])\n",
        "                else:\n",
        "                  text= limpiar_texto(item['text'])\n",
        "                  contenido += f\"**{item['heading']}**\\n\"+ text+ \"\\n\"\n",
        "\n",
        "            texto= item['text'] #re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            texto = limpiar_texto(texto)\n",
        "            cadena=cadena+texto+\"\\n\"\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": cadena,\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": introduccion,\n",
        "                      \"Contenido\": contenido,\n",
        "                      \"Conclusion\": conclusion,\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "        else:\n",
        "          print(\"Archivo \"+nombre +\" sin contenido\")\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": \"\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": \"\",\n",
        "                      \"Contenido\": \"\",\n",
        "                      \"Conclusion\": \"\",\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "df.to_csv(ruta_carpeta_principal+\"/\"+\"arxivAi_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8-sig')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal+\"/\"+\"arxivAi_primer_ciclo.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UN1mqL4J5qph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtener estado de aceptacion"
      ],
      "metadata": {
        "id": "v9EqF9IqeZUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part = [\"dev/reviews/\", \"test/reviews/\", \"train/reviews/\"]\n",
        "lista_conf = [\"arxiv.cs.ai_2007-2017\"]\n",
        "csv_path = os.path.join(ruta_carpeta_principal, \"reviews_arxivAi.csv\")\n",
        "\n",
        "# Crear o cargar el DataFrame desde el archivo CSV existente\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"Archivo\", \"Estado\"])\n",
        "\n",
        "dataframes = []  # Lista para almacenar los DataFrames temporales\n",
        "\n",
        "for confe in lista_conf:\n",
        "    for particion in lista_part:\n",
        "        ruta = os.path.join(ruta_carpeta_principal, confe, particion)\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        for archivo in archivos:\n",
        "            archivo_path = os.path.join(ruta, archivo)\n",
        "\n",
        "            with open(archivo_path, 'r') as f:\n",
        "                contenido = json.load(f)\n",
        "                id = contenido[\"id\"]\n",
        "                id = id+'.pdf'\n",
        "                accepted = contenido[\"accepted\"]\n",
        "\n",
        "                # Verificar si accepted es un booleano y asignar 1 o 0 en consecuencia\n",
        "                if isinstance(accepted, bool):\n",
        "                    accepted_value = \"1\" if accepted else \"0\"\n",
        "                else:\n",
        "                    accepted_value = \"1\" if accepted.lower() == \"verdadero\" else \"0\"\n",
        "\n",
        "                temp_df = pd.DataFrame({\"Archivo\": [id], \"Accepted\": [accepted_value]})\n",
        "                dataframes.append(temp_df)\n",
        "\n",
        "# Concatenar todos los DataFrames temporales en uno solo\n",
        "if dataframes:\n",
        "    df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame actualizado en el archivo CSV\n",
        "df.to_csv(csv_path, sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", csv_path)\n"
      ],
      "metadata": {
        "id": "8SOZMxDK6QPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#combinar csv"
      ],
      "metadata": {
        "id": "NT1MTWJ5l8QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"arxivAi_primer_ciclo.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"reviews_arxivAi.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Guarda el número inicial de filas en cada DataFrame\n",
        "num_filas_inicial_df1 = len(df1)\n",
        "print(\"Filas del doc 1:\",num_filas_inicial_df1)\n",
        "num_filas_inicial_df2 = len(df2)\n",
        "print(\"Filas del doc 2:\",num_filas_inicial_df2)\n",
        "\n",
        "# Union de csv con base en la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "print(\"Número total de elementos al unir:\", len(result))\n",
        "\n",
        "# Dropear filas con valores nulos en columnas\n",
        "result = result.dropna(subset=['Accepted'])\n",
        "#result = result.dropna(subset=['Contenido'])\n",
        "print(\"Número total de elementos despues de dropear:\", len(result))\n",
        "# Calcular la diferencia en el numero de filas despues de la limpieza\n",
        "\n",
        "\n",
        "#print(result)\n",
        "# Guarda el resultado en un CSV\n",
        "result.to_csv('/content/drive/MyDrive/arxivAi_datos_completos.csv', index=False)\n",
        "print(result)\n",
        "\n",
        "# Imprime el numero total de filas eliminadas\n",
        "#print(\"Número total de filas eliminadas:\", num_total_filas_eliminadas)\n",
        "\n"
      ],
      "metadata": {
        "id": "n0FlQ7rBWFFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}